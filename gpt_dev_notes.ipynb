{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMT6SzNTQTOOvgIWTnvaHnP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szhou12/gpt-from-scratch/blob/main/gpt_dev_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT From Scratch\n",
        "## Resources\n",
        "- [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy)\n",
        "- [Andrej Karpathy《从零开始搭建GPT|Let's build GPT from scratch, in code, spelled out》中英字](https://www.bilibili.com/video/BV1v4421c7fr/?spm_id_from=333.337.search-card.all.click&vd_source=0c02ef6f6e7a2b0959d7dd28e9e49da4)"
      ],
      "metadata": {
        "id": "AOdPQe0Xgp85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oexPrYy6hLV8",
        "outputId": "649d6f45-7084-4643-88c2-0a68f8455a92"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-21 08:23:59--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-02-21 08:23:59 (20.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read text data in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "d9M8pu4ahU-i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHjNfGaAjFP8",
        "outputId": "bb39ee93-06cd-4938-a1a8-2090a0c2123c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1,000 chars\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsmE6THXjJCP",
        "outputId": "e8f3e02c-5a23-4a81-81d5-3a569224c646"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all the unique characters that occur in the text\n",
        "## set(text): make the set of all unique chars in text data\n",
        "## list(...): to have some ordering so that we can sort it\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okEGKmicjPFq",
        "outputId": "c1bebd22-cae5-45fd-d82d-a696a24a12cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize [00:09:30]\n",
        "- Tokenization: to convert the raw text as string into some sequences of integers according to some notebooks/rules/vocabularies of elements (字符串转换成数字序列的mapping过程).\n",
        "- 下面给出的例子中, tokenization rule就是根据每个字符对应的index来进行编码.\n",
        "- 常用的Tokenization方法:\n",
        "  1. Byte-Pair Encoding (BPE)\n",
        "    - [Byte-Pair Encoding tokenization - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter6/5)\n",
        "  2. SentencePiece by Google\n",
        "  3. tiktoken by OpenAI\n",
        "- One important Observation: trade-off between codebook size and sequence length.\n",
        "  - The smaller the vobabulary size, the longer the sequences of integers.\n",
        "    - e.g. character level vocab is small, so the encoded seq of ints will be long. (shown below)\n",
        "  - The larger the vobabulary size, the shorter the sequences of integers.\n",
        "    - e.g. sub-word level vocab is large, so the encoded seq of ints will be short."
      ],
      "metadata": {
        "id": "QVQ7hs4kkHQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUpXCVD2jqfx",
        "outputId": "2ad421a0-bcc2-4c64-bb5b-356e3ca4bbde"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long) # 1-D list\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # show first 1000 characters we looked at earlier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veS-EFnUmSAX",
        "outputId": "5fd9abf0-4301-4e17-f3bb-c4cdd3be3ad3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train & Test Split [00:13:42]"
      ],
      "metadata": {
        "id": "yN2UvAKLp9i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now split up the data into train and validation sets\n",
        "n = int(0.9 * len(data)) # first 90% chars will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:] # use val set to get a sense of overfitting"
      ],
      "metadata": {
        "id": "Kl8EUf8CqFAg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train On Chunks [00:14:28]\n",
        "- **Important thing to notice**:\n",
        "  1. we'll never feed the entire text into the transformer all at once because it's computationally expensive.\n",
        "  2. We only feed chunks of the text.\n",
        "  3. Ramdomly sampling chunks from the text dataset and train on a chunk at a time.\n",
        "  4. These chunks will be pre-set with max length (normally called `block_size` or `context_length`)."
      ],
      "metadata": {
        "id": "qho16AhEqy0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set block_size = 8\n",
        "block_size = 8\n",
        "train_data[:block_size+1] # first 9 chars. Do you understand why 9 intead of 8?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo_-Fy0CqdJD",
        "outputId": "c0903f78-e911-4a66-b777-54771f2ab8b3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- How many training examples are there in this sequence of integers of length=9?\n",
        "  - 8 examples!!!\n",
        "   - Because to predict `i`-th position's char (called \"target\"), we need to use all `[0:i-1]` positions' chars (called \"context\"). i.e. one example = `[0:i-1]` predict `[i]`\n",
        "  - [18] predicts 47, [18, 47] predict 56, [18, 47, 56] predict 58, ...\n",
        "  - The code below illustrate this concept:"
      ],
      "metadata": {
        "id": "1NImim_utC8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"Round {t}: when input (aka. context) is {context}, the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfwyJRLSsLdO",
        "outputId": "4fe09b1a-6a5a-45f7-eefc-c877fdd6c525"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 0: when input (aka. context) is tensor([18]), the target: 47\n",
            "Round 1: when input (aka. context) is tensor([18, 47]), the target: 56\n",
            "Round 2: when input (aka. context) is tensor([18, 47, 56]), the target: 57\n",
            "Round 3: when input (aka. context) is tensor([18, 47, 56, 57]), the target: 58\n",
            "Round 4: when input (aka. context) is tensor([18, 47, 56, 57, 58]), the target: 1\n",
            "Round 5: when input (aka. context) is tensor([18, 47, 56, 57, 58,  1]), the target: 15\n",
            "Round 6: when input (aka. context) is tensor([18, 47, 56, 57, 58,  1, 15]), the target: 47\n",
            "Round 7: when input (aka. context) is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **One More Important Thing**\n",
        "  - Why, for a given block size (=8 in this exmaple), we train from context length of 1 all the way up to context length of block_size (=8)?\n",
        "  - Not only for computational efficiency;\n",
        "  - But in order for the transformer to be used to seeing context from as little as length 1 all the way to block size. That is, we want the transformer to be used to all these context lengths (1, 2, 3, ..., 8).\n",
        "    - Why? Because doing so is useful for later inference.\n",
        "    - How so? Because while we start sampling, we can start the sampling generation with context length = 1. The transformer will know how to predict in this situation. Similarly, the transformer will how how to predict in the situation of context length = `2, 3, ..., block_size`.\n",
        "    - 简言之，让模型习惯应对任意长度(`1,2,3, ..., block_size`)的输入情况下的预测。"
      ],
      "metadata": {
        "id": "PNwOnpRjvPhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch dimension [00:18:07]\n",
        "- Notice that, so far, the tensor is 1-D (call it \"time dimension\").\n",
        "- Now onto batch dimension! Every time of sampling, we retreive a batch of chunks instead of one chunk, and feed this batch (multiple chunks) into the transformer all at once at the same time.\n",
        "- Why? Mainly for computational efficiency. Because GPU is good at parallel processing of data.\n",
        "- But notice! Each chunk from the batch is processed independently! They don't talk to each other!"
      ],
      "metadata": {
        "id": "zCdZGAtlyHEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we parallelly process in every forward-backward pass in the transformer?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs X and targets y\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # 从[0, n-block_size)中随机抽4个数，返回长度为4的list [a, b, c, d]. (batch_size=4,)输出1-D list\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]) # X\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # true y - used to calculate loss y - y_hat\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size):\n",
        "    for t in range(block_size):\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b, t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaSkFOqZurDx",
        "outputId": "824068f9-1064-4d8e-ddea-37ce8b3662af"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feeding Data to Neural Network - Bigram Language Model [00:22:14]\n",
        "- Start with a simple neural network: Bi-gram language model (details in his previous courses)"
      ],
      "metadata": {
        "id": "u8mjRHNL32XB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch Recap\n",
        "1. `import torch.nn as nn`\n",
        "    - The `torch.nn` module contains classes and functions that are used for building neural networks in PyTorch. This includes the foundational building blocks for neural networks, such as layers (e.g., `Linear`, `Conv2d`), and modules (`Module`), which are the base class for all neural network modules.\n",
        "    - `nn` (Object-oriented API): Provides classes that allow you to encapsulate parameters and helpers in objects. This is useful for defining complex models and is typically preferred when designing architectures.\n",
        "2. `from torch.nn import functional as F`\n",
        "    - The `torch.nn.functional` module contains function versions of many of the operations and layers available in `torch.nn`.\n",
        "    - These functions include activation functions (`F.relu`, `F.sigmoid`), operations used in convolutional neural networks (`F.conv2d`, `F.max_pool2d`), and loss functions (`F.cross_entropy`), among others.\n",
        "    - Unlike the classes in `torch.nn`, which require creating an instance of the class (e.g., `layer = nn.Conv2d(...)`), functions in `torch.nn.functional` can be used directly by passing inputs and any necessary parameters (e.g., `output = F.conv2d(input, weight)`).\n",
        "    - `F` (Functional API): Provides stateless, functional alternatives to the classes in `nn`. This is useful for operations that don't require storing state (parameters), such as applying activations, performing a convolution operation with dynamically created filters, or applying a loss function directly within the model's forward method.\n",
        "3. `nn.Embedding`\n",
        "    - A PyTorch layer that's typically used to convert token indices in a vocabulary into dense vector representations (embeddings). The layer takes two main arguments: **(the number of embeddings, the dimensionality of each embedding vector)**. In most natural language processing (NLP) tasks, the dimensionality of embeddings is much smaller than the vocabulary size, facilitating efficient representation of words or tokens.\n",
        "    - Assume `vocab_size=65`, This layer (`self.token_embedding_table`) now acts as a 65x65 table where **each row corresponds to a token** and **each column to a possible next token**, **with values being the logits** (raw predictions prior to normalization).\n",
        "    - When you input an index (or indices) to this layer, it returns the corresponding row(s) (aka. channel) from the table. For instance, if the input index is 3 (for `a`), the layer returns the 4th row (0-indexed) of the table, which is a vector of size 65. Each element of this vector represents the model's logit for the probability of each vocabulary token being the next token after `a`.\n",
        "4. Why Use Cross-Entropy For Loss?\n",
        "    - Recall cross-entropy measures the difference between two probability distributions."
      ],
      "metadata": {
        "id": "pQUFhLnB5zLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token direclty reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        '''\n",
        "        idx: (B,T) tensor of integer. data. xb from previous cell\n",
        "        targets (Optional): (B,T) tensor of integer. true y. yb from previous cell. Make it optional as generate() won't use loss.\n",
        "        '''\n",
        "\n",
        "        # idx and targets are both (B, T) tensor of integers\n",
        "        # 相当于, 根据idx对应的输入抽出所有下一个token可能出现的概率, 所有下一个token称为channel, 长度为vocab_size\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C) = (batch, time, channel/class=vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # 进行维度的转换，因为Pytorch中cross_entropy()的第二个param只能接收C, 保留原来的(B,T,C)会报错\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C) # squash first 2 dims into 1 dim, keep the 3rd dim\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        '''\n",
        "        idx: (B, T) array of indices in the current context.\n",
        "            Note! indices meant here are integer representations of chars, so essentially idx are data (context) we feed\n",
        "        max_new_tokens: # of new tokens to be predicted and appended to idx\n",
        "        '''\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions (logits)\n",
        "            logits, loss = self(idx) # self(idx) calls forward()\n",
        "\n",
        "            # focus only on the last time step\n",
        "            # -1意味着我们现在只在意用8个token一起predict的那个example\n",
        "            # 注意: logits[:, i, :] ith位子上的logit表示用前i个token一起predict下一个可能出现的token\n",
        "            # e.g. 1st logit是只用第一个token predict的结果, 2nd logit是用头两个token predict的结果, 依此类推.\n",
        "            # -1表示最后一个位子上的logit, 用了所有前面能用到的token\n",
        "            # !!!上面的理解错了!!!\n",
        "            # -1意味着我们现在只用最后一个token来predict下一个token, 没有用到再往前的tokens!\n",
        "            # 注意: logits[:, i, :] ith位子上的logit表示用第i个token来predict下一个可能出现的token\n",
        "            # e.g. 1st logit是只用第一个token predict的结果, 2nd logit只用第二个token predict的结果, 依此类推.\n",
        "            # -1表示最后一个位子上的logit, 只用了所有最后一个token\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "\n",
        "            # apply softmax to get the probablities\n",
        "            # dim=-1: 沿着last dimension of logits (ie. C)计算softmax score\n",
        "            # 每条data, 横向地沿着所有classes计算softmax. 这样，batch中每一条data都独立计算\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            # 每条data, 根据class probabilities随机采样一个class.\n",
        "            # e.g. 假设一条data的probability: [0.1, 0.2, 0.5, 0.3], multinomial根据每个class的概率大小抽一个class (num_samples=1)\n",
        "            # 显然, i=2的元素最容易抽到因为它的概率最高=0.5\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            # dim=1 沿着第二个dimension (sequence length dimension)，一列一列地append新的采样到的data\n",
        "            # [[24, 43], -> [[24, 43, 58],\n",
        "            #  [52, 58]] ->  [52, 58,  1]]\n",
        "            # 注意！这里可以直接append index, 是因为tokenization是index直接对应char. 如果是其他tokenize方法, 可能需要额外转换index对应的元素\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "\n",
        "\n",
        "# 1. instantiate BigramLanguageModel class;\n",
        "# 2. init token_embedding_table as an embedding layer, with logits randomly generated\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "# Given [[0]] (1 batch, 1 time step), sequentially generate 100 tokens after first token: [[0]] -> [[0, 1]] -> [[0, 1, 3]] -> ...\n",
        "# Remember, in this case, each token newly generated uses ONLY ONE token generated ahead of it. NO HISTORY USED HERE! Can you recall why?\n",
        "# [0].tolist(): take out the content of the first data from the batch and convert it to list\n",
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddtomesx0iqz",
        "outputId": "8385795d-6933-41bf-9fd7-20da78214a6b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train The Bigram Language Model [0:34:50]\n",
        "\n"
      ],
      "metadata": {
        "id": "r2QXnL9qFKQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `AdamW`: A variant of the Adam optimizer that decouples the weight decay from the optimization steps. Currently a popular choice of optimizer.\n",
        "2. `lr=1e-3`: Typically, a good learning rate = `3e-4`. But if you're using small models, you can use higher learning rate."
      ],
      "metadata": {
        "id": "ULs3HL9-zRcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "##\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "HIVKsY0jACkl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below provides a template of training:\n",
        "1. `optimizer.zero_grad(set_to_none=True)`: **Zeroing the Gradients**. Clear out the gradients from the previous training step before using optimizer to update the model parameters by this round's gradients. In Pytorch, the gradients are accumulated through training steps!\n",
        "    - `set_to_none=True`: aims to improve the speed of setting gradients to zero. Traditional way is `optimizer.zero_grad()`, which iterates through all model params' `.grad` attributes and set to zero. Instead of filling zeros, `set_to_none=True` sets `.grad` to `None`. Two advantages: 1) It reduces memory usage 因为不需要内存来暂时储存0. 2) It can potentially speed up gradient zeroing 因为设为None比填充0快一些\n",
        "2. `loss.backward()`: computes the gradient of the loss w.r.t the model parameters using backpropagation.\n",
        "3. `optimizer.step()`: updates the model parameters based on the gradients calculated. Update rule depends on the optimizer choice (In this case is `AdamW`).\n",
        "4. `loss.item()`: `.item()` method converts the loss from a tensor to a Python scalar."
      ],
      "metadata": {
        "id": "kABXn4G-zpYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "iters = 100 # increase # of steps for better results\n",
        "for steps in range(iters):\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb) # Forward pass + Loss\n",
        "    optimizer.zero_grad(set_to_none=True) # clear out previous gradient\n",
        "    loss.backward() # Gradient\n",
        "    optimizer.step() # Update w\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "d100HfkhGC8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ceb291-1f0e-4842-acac-11be38233c3d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.587916374206543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo of Typical Training Process\n",
        "- Model: a very simple neural network on a regression task $y = wx + b$.\n",
        "- Parameters: $w$ and $b$ (bias). Initially, $w=0$, $b=0$. Assume $b=0$ throughout the training.\n",
        "- Loss Function: $L = (y-\\widehat{y})^{2}$ (MSE)\n",
        "- Learning Rate: $lr = 0.01$\n",
        "- Dataset: $(x_1=2, y_1=4)$, $(x_2=3, y_2=6)$\n",
        "\n",
        "1. First step of training using $(x_1=2, y_1=4)$:\n",
        "    1. Forward pass: $\\widehat{y_1} = w⋅x_1 = 0⋅2 = 0$\n",
        "    2. Loss: $L_1 = (y_1-\\widehat{y_1})^{2} = (4-0)^2 = 16$\n",
        "    3. Gradient: $\\frac{dL_1}{dw} = 2⋅(y_1-\\widehat{y_1})⋅(-x_1) = 2⋅(4-0)\\cdot(-2) = -16$\n",
        "    4. Update $w$: $w=w-lr⋅\\frac{dL_1}{dw} = 0-0.01⋅(-16)=0.16$\n",
        "2. Second step of training using $(x_2=3, y_2=6)$:\n",
        "    1. Forward pass: $\\widehat{y_2} = w⋅x_2 = 0.16⋅3 = 0.48$\n",
        "    2. Loss: $L_2 = (y_2-\\widehat{y_2})^{2} = (6-0.48)^2$\n",
        "    3. Gradient: $\\frac{dL_2}{dw} = 2⋅(y_2-\\widehat{y_2})⋅(-x_2) = 2⋅(6-0.48)⋅(-3) = -33.12$\n",
        "        - Note: here we clear out step 1's gradient so that we use  $\\frac{dL_2}{dw}$ instead of $\\frac{dL_2}{dw}+\\frac{dL_1}{dw}$!!!\n",
        "    4. Update $w$: $w=w-lr⋅\\frac{dL_2}{dw} = 0.16-0.01⋅(-33.12)=0.4912$"
      ],
      "metadata": {
        "id": "Si8aPFax3DVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xe3JYK29PlX",
        "outputId": "879e1591-58da-4e52-e146-108acce739af"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "xiKi-RJ:CgqVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&-gPlLyulId?XlaInQ'q,lT$\n",
            "3Q&sGlvHQ?mqSq-eON\n",
            "x?SP fUAfCAuCX:bOlgiRQWN:Mphaw\n",
            "tRLKuYXEaAXxrcq-gCUzeh3w!AcyaylgYWjmJM?Uzw:inaY,:C&OECW:vmGGJAn3onAuMgia!ms$Vb q-gCOcPcUhOnxJGUGSPJWT:.?ujmJFoiNL&A'DxY,prZ?qdT;hoo'dHooXXlxf'WkHK&u3Q?rqUi.kz;?Yx?C&u3Qbfzxlyh'Vl:zyxjKXgC?\n",
            "lv'QKFiBeviNxO'm!Upm$srm&TqViqiBD3HBP!juEOpmZJyF$Fwfy!PlvWPFC\n",
            "&WDdP!Ko,px\n",
            "x\n",
            "tREOE;AJ.BeXkylOVD3KHp$e?nD,.SFbWWI'ubcL!q-tU;aXmJ&uGXHxJXI&Z!gHRpajj;l.\n",
            "pTErIBjx;JKIgoCnLGXrJSP!AU-AcbczR?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Put All Above Into `bigram.py` [00:38:00] With 2 Updates\n",
        "1. Additional code to run on GPU device\n",
        "2. Introduce `estimate_loss()` to average out noisy loss resulted from randomly sampled batches (some batch lucky, some not). [00:39:40]"
      ],
      "metadata": {
        "id": "O1aisook-cMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Math Trick in Self-Attention [00:42:14]"
      ],
      "metadata": {
        "id": "TXRtTjHuCNVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improve Using The Trick: bigram_v2.py [00:58:28]\n",
        "1. Take out `vocab_size` as input arg of `BigramLanguageModel`, making it global variable.\n",
        "2. Inroduce a new global variable `n_embd`.\n",
        "3. In `forward()`, replace `logits` with `tok_emb` in embedding layer.\n",
        "4. Add a linear layer in the constructor.\n",
        "5. Add a position embedding layer below token embedding layer and above linear layer. Position embedding layer encodes position info of tokens."
      ],
      "metadata": {
        "id": "ZI0mESIwGZc8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-lUEOcD7968v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}