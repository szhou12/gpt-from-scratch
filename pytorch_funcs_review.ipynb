{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlxpMzKSRFHaqsC0dLA26w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szhou12/gpt-from-scratch/blob/main/pytorch_funcs_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xzsZBomJXNml"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "- `nn.Embedding(n, d)`: an Embedding module (table) containing `n` tensors of size `d`.\n",
        "- use `.weight` to show content of embedding table.\n",
        "- Official Doc: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "\n",
        "## 用法总结\n",
        "1. Declare phase\n",
        "    - 声明一个 embedding table: `token_embedding_table = nn.Embedding(vocab_size, n_embd)`\n",
        "    - `vocab_size`: 表示语料库/text中出现的所有单词/token的种类。可以理解成“字典”。\n",
        "    - `n_embd`: 对于每一种单词/token，向量化成长度=`n_embd`的vector。这样才能进行数学计算。\n",
        "2. Use phase\n",
        "    - 对实际进来的data进行embedding: `token_embd = token_embedding_table(Xb)`\n",
        "    - `Xb`: 假设是`(B, T)`，表示进来B条文本，每条文本有T个单词/token。显然，`Xb[i][j]`表示第i条文本中，第j号位置上的单词/token\n",
        "    - 对于任何一个`(i, j)`位置上的单词/token，我们都存在了“字典”`vocab_size`中。embedding的过程就是拿着每一个位置上的单词/token，去找对应的embedding vector。\n",
        "    - 所以，`token_embd`的最终形状为 `(B, T, n_embd)`。实际上，就是把`Xb`中每一个单词/token，都幻化成vector。\n",
        "3. Addition\n",
        "    - `tok_emb + pos_emb = (B, T, C) + (T, C) = (B, T, C) + B * (T, C) = (B, T, C)`\n",
        "    - 相当于，把`pos_emb`复制 B 份，然后进行matrix element-wise addition"
      ],
      "metadata": {
        "id": "WvqKCXgeXRrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Declare Phase\n",
        "# an Embedding module containing vocab_size=10 tensors of size n_embd=3\n",
        "token_embedding_table = nn.Embedding(10, 3)\n",
        "print(\"-----embedding lookup table-----\")\n",
        "print(token_embedding_table.weight)\n",
        "\n",
        "## Use Phase\n",
        "# a batch of B=2 samples of T=4 indices each\n",
        "input = torch.LongTensor([[1, 2, 4, 5],\n",
        "                          [4, 3, 2, 9]])\n",
        "tok_emb = token_embedding_table(input)\n",
        "\n",
        "print(\"-----actually embed data-----\")\n",
        "print(tok_emb)\n",
        "print(\"-----shape of data after embed-----\")\n",
        "print(tok_emb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST2uTHQBuNSd",
        "outputId": "b564eafa-2a54-4540-a60f-492d6978b64e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----embedding lookup table-----\n",
            "Parameter containing:\n",
            "tensor([[ 1.7884,  0.1894, -1.3711],\n",
            "        [-0.4990,  0.4098, -0.4139],\n",
            "        [-2.3013, -0.9812,  1.6015],\n",
            "        [-0.5372, -1.2943, -1.2302],\n",
            "        [-0.7367,  0.3864, -0.5681],\n",
            "        [ 0.7724,  1.7569, -1.6473],\n",
            "        [ 0.5956, -0.3517, -0.3045],\n",
            "        [-1.1574, -0.5102,  0.6259],\n",
            "        [ 1.4290, -0.8259,  0.9965],\n",
            "        [ 0.5158,  0.9047,  1.8139]], requires_grad=True)\n",
            "-----actually embed data-----\n",
            "tensor([[[-0.4990,  0.4098, -0.4139],\n",
            "         [-2.3013, -0.9812,  1.6015],\n",
            "         [-0.7367,  0.3864, -0.5681],\n",
            "         [ 0.7724,  1.7569, -1.6473]],\n",
            "\n",
            "        [[-0.7367,  0.3864, -0.5681],\n",
            "         [-0.5372, -1.2943, -1.2302],\n",
            "         [-2.3013, -0.9812,  1.6015],\n",
            "         [ 0.5158,  0.9047,  1.8139]]], grad_fn=<EmbeddingBackward0>)\n",
            "-----shape of data after embed-----\n",
            "torch.Size([2, 4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Declare Phase\n",
        "# T=4, n_embd=3\n",
        "position_embedding_table = nn.Embedding(4, 3)\n",
        "\n",
        "## Use Phase\n",
        "# vectorize 0-th, 1-th, 2-th, 3-th positions as size=3 vectors respectively\n",
        "pos_emb = position_embedding_table(torch.arange(4))\n",
        "\n",
        "print(\"-----actually embed positions-----\")\n",
        "print(pos_emb)\n",
        "print(\"-----shape of positions after embed-----\")\n",
        "print(pos_emb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQFrZlt59Bee",
        "outputId": "88760913-4a1a-45e5-a350-e0c68975ec4f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----actually embed positions-----\n",
            "tensor([[ 0.0659, -0.7604, -0.6842],\n",
            "        [-0.2087, -0.4915, -1.2146],\n",
            "        [ 0.2580,  2.1900,  0.2527],\n",
            "        [-1.1342, -0.3356, -0.3737]], grad_fn=<EmbeddingBackward0>)\n",
            "-----shape of positions after embed-----\n",
            "torch.Size([4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Addition of Embedding Tables\n",
        "# how to align and broadcast: (B, T, n_embd) + (T, n_embd)\n",
        "x = tok_emb + pos_emb\n",
        "print(\"-----(B, T, n_embd) + (T, n_embd)-----\")\n",
        "print(x)\n",
        "print(\"-----shape of (B, T, n_embd) + (T, n_embd)-----\")\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYec17_J-xXM",
        "outputId": "13bfd8f3-a186-440d-e613-011f91e96b28"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----(B, T, n_embd) + (T, n_embd)-----\n",
            "tensor([[[-1.3533,  1.6135,  2.2122],\n",
            "         [-3.4185, -2.3436,  2.5595],\n",
            "         [-0.7281, -0.1098, -0.0819],\n",
            "         [ 1.1439,  0.9010, -1.5603]],\n",
            "\n",
            "        [[-1.5911,  1.5901,  2.0580],\n",
            "         [-1.6545, -2.6567, -0.2722],\n",
            "         [-2.2926, -1.4774,  2.0877],\n",
            "         [ 0.8873,  0.0487,  1.9009]]], grad_fn=<AddBackward0>)\n",
            "-----shape of (B, T, n_embd) + (T, n_embd)-----\n",
            "torch.Size([2, 4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# .to(device)\n",
        "- Move the model's parameters (weights and biases) to a specified computing device (e.g. GPU).\n",
        "- In-place operation for `nn.Module` objects, meaning `model` itself is moved to the device.\n",
        "- It's common practice to write as `m = model.to(device)`. However, `m` is just another reference as `model`, meaning they are both moved to the same device. This line could be simplified to just `model.to(device)` instead of `m = model.to(device)` if the separate reference `m` is not specifically needed for later use."
      ],
      "metadata": {
        "id": "2AIFbxI4soXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Runtime -> Change runtime type -> select 'T4 GPU' to use 'cuda'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Example model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.linear = nn.Linear(10, 5)  # A simple linear layer\n",
        "\n",
        "model = SimpleModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# Check: Iterate through all parameters in 'model' and print their device\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"model: {name} is on {param.device}\")\n",
        "\n",
        "# Check: Iterate through all parameters in 'm' and print their device\n",
        "for name2, param2 in m.named_parameters():\n",
        "    print(f\"m: {name2} is on {param2.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZbPX-tgqTG0",
        "outputId": "65507446-4b92-4368-acc6-67cbfa07aaa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: linear.weight is on cuda:0\n",
            "model: linear.bias is on cuda:0\n",
            "m: linear.weight is on cuda:0\n",
            "m: linear.bias is on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6p7jwJmi2HZZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}